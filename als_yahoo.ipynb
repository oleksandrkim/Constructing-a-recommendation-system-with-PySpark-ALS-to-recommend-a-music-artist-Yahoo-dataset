{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constructing a recommendation system with PySpark (ALS) to recommend a music artist (Yahoo dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Information about the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Number of inputs: >150 000 000, for the project only 100,000 are used\n",
    "- Total number of music artists: 97 956\n",
    "- Dataset: https://webscope.sandbox.yahoo.com/catalog.php?datatype=r&did=1 <br>\n",
    "A permission to use this data set for non-commercial usage was provided by Yahoo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Initializing Spark and importing libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "import pyspark\n",
    "import numpy\n",
    "import pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"rec\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.ml.evaluation import RegressionEvaluator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Import of a dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+---+\n",
      "|_c0|    _c1|_c2|\n",
      "+---+-------+---+\n",
      "|  1|1000125| 90|\n",
      "|  1|1006373|100|\n",
      "|  1|1006978| 90|\n",
      "|  1|1007035|100|\n",
      "|  1|1007098|100|\n",
      "+---+-------+---+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = spark.read.csv(\"data/part1.txt\", inferSchema=True,header=False).limit(100000)\n",
    "data.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data preprocessing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- UserID: integer (nullable = true)\n",
      " |-- ArtistID: integer (nullable = true)\n",
      " |-- score: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "df = data.select(col(\"_c0\").alias(\"UserID\"), col(\"_c1\").alias(\"ArtistID\"), col(\"_c2\").alias(\"score\"))\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+-----------------+-----------------+\n",
      "|summary|            UserID|         ArtistID|            score|\n",
      "+-------+------------------+-----------------+-----------------+\n",
      "|  count|            100000|           100000|           100000|\n",
      "|   mean|          807.9369|    1033841.04656|         56.18287|\n",
      "| stddev|501.30436056253603|32723.43135117443|40.57942182261647|\n",
      "|    min|                 1|            24538|                0|\n",
      "|    max|              1675|          1101671|              255|\n",
      "+-------+------------------+-----------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Min-Max Scaler applied on scores**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "assembler = VectorAssembler(inputCols=[\"score\"],\n",
    "                            outputCol=\"score_f\")\n",
    "\n",
    "df_t = assembler.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+-----+-------+\n",
      "|UserID|ArtistID|score|score_f|\n",
      "+------+--------+-----+-------+\n",
      "|     1| 1000125|   90| [90.0]|\n",
      "|     1| 1006373|  100|[100.0]|\n",
      "|     1| 1006978|   90| [90.0]|\n",
      "|     1| 1007035|  100|[100.0]|\n",
      "|     1| 1007098|  100|[100.0]|\n",
      "+------+--------+-----+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_t.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import MinMaxScaler\n",
    "from pyspark.ml.linalg import Vectors\n",
    "\n",
    "\n",
    "scaler = MinMaxScaler(inputCol=\"score_f\", outputCol=\"scaled_score\")\n",
    "\n",
    "# Compute summary statistics and generate MinMaxScalerModel\n",
    "scalerModel = scaler.fit(df_t)\n",
    "\n",
    "# rescale each feature to range [min, max].\n",
    "scaled_df = scalerModel.transform(df_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+-----+-------+--------------------+\n",
      "|UserID|ArtistID|score|score_f|        scaled_score|\n",
      "+------+--------+-----+-------+--------------------+\n",
      "|     1| 1000125|   90| [90.0]|[0.35294117647058...|\n",
      "|     1| 1006373|  100|[100.0]|[0.39215686274509...|\n",
      "|     1| 1006978|   90| [90.0]|[0.35294117647058...|\n",
      "|     1| 1007035|  100|[100.0]|[0.39215686274509...|\n",
      "|     1| 1007098|  100|[100.0]|[0.39215686274509...|\n",
      "+------+--------+-----+-------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "scaled_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features scaled to range: [0.000000, 1.000000]\n",
      "+-------+--------------------+\n",
      "|score_f|        scaled_score|\n",
      "+-------+--------------------+\n",
      "| [90.0]|[0.35294117647058...|\n",
      "|[100.0]|[0.39215686274509...|\n",
      "| [90.0]|[0.35294117647058...|\n",
      "|[100.0]|[0.39215686274509...|\n",
      "|[100.0]|[0.39215686274509...|\n",
      "+-------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Features scaled to range: [%f, %f]\" % (scaler.getMin(), scaler.getMax()))\n",
    "scaled_df.select(\"score_f\", \"scaled_score\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Converting a vector to Double**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import DoubleType\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "new_df = scaled_df.withColumn(\"label\", scaled_df.scaled_score.cast(StringType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df1 = new_df.withColumn('label_l', regexp_replace('label', ']', ''))\n",
    "#new_df2 = new_df1.withColumn('label_l', regexp_replace('label', '[', ''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = new_df1.withColumn(\"scores_d\", new_df1.label_l.substr(2, 10).cast(DoubleType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- UserID: integer (nullable = true)\n",
      " |-- ArtistID: integer (nullable = true)\n",
      " |-- score: integer (nullable = true)\n",
      " |-- score_f: vector (nullable = true)\n",
      " |-- scaled_score: vector (nullable = true)\n",
      " |-- label: string (nullable = true)\n",
      " |-- label_l: string (nullable = true)\n",
      " |-- scores_d: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nn.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_r = nn.select([\"UserID\",\"ArtistID\",\"scores_d\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_r.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Train-test split**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = df_r.randomSplit([0.8, 0.2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Initialize of the ALS model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "als = ALS(userCol='UserID', itemCol='ArtistID', ratingCol='scores_d', coldStartStrategy='drop')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Usage of ParGridBuilder to find the best combination of parameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import ParamGridBuilder, TrainValidationSplit\n",
    "\n",
    "paramGrid = ParamGridBuilder()\\\n",
    "    .addGrid(als.regParam, [0.1, 0.01]) \\\n",
    "    .addGrid(als.maxIter, [5, 10])\\\n",
    "    .addGrid(als.rank, [10, 20])\\\n",
    "    .build()\n",
    "\n",
    "\n",
    "#some other regularization parameters:\n",
    "#.addGrid(als.alpha, [0.1, 0.5, 1])\\\n",
    "#.addGrid(als.nonnegative, [False, True])\\\n",
    "#.addGrid(als.implicitPrefs, [False, True])\\"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cross-validation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "\n",
    "crossval = CrossValidator(estimator=als,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=RegressionEvaluator(metricName='rmse', labelCol='scores_d', \n",
    "                                                        predictionCol='prediction'),\n",
    "                          numFolds=2)  # use 3+ folds in practice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fit training data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvModel = crossval.fit(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Extract the best model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = cvModel.bestModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Generate predictions on test data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = best_model.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+----------+------------+\n",
      "|UserID|ArtistID|  scores_d|  prediction|\n",
      "+------+--------+----------+------------+\n",
      "|    91| 1015250|       0.0| -0.15642607|\n",
      "|  1114| 1015250|0.35294117|  0.33530283|\n",
      "|  1206| 1015250|0.39215686|  0.35214406|\n",
      "|   868| 1015250| 0.2745098|  0.06059181|\n",
      "|   542| 1015250|0.31764705|  0.33039916|\n",
      "|    51| 1015250|0.39215686|  0.08423141|\n",
      "|  1280| 1017864|0.10980392|  0.11224718|\n",
      "|   232| 1017864|       0.0| 0.027070649|\n",
      "|   140| 1017864|       0.0|  0.10479112|\n",
      "|   205| 1017864| 0.2745098|  0.13553531|\n",
      "|  1487| 1017864|       0.0| 0.010085652|\n",
      "|  1248| 1017864|0.19607843|  0.21532978|\n",
      "|    49| 1017864|0.35294117|  0.23889358|\n",
      "|    51| 1017864|       0.0|0.0077098743|\n",
      "|   957| 1017864|0.31372549|   0.3157921|\n",
      "|   143| 1017864|       0.0|0.0031054735|\n",
      "|  1668| 1017864|       0.0|  0.03946184|\n",
      "|   608| 1017864| 0.2745098|   0.2274194|\n",
      "|   541| 1017864|       0.0| 0.088495895|\n",
      "|   566| 1017864|0.35294117|   0.3175629|\n",
      "+------+--------+----------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Use RMSE to evaluate the model on test data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = RegressionEvaluator(metricName='rmse', labelCol='scores_d', predictionCol='prediction')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse = evaluator.evaluate(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.11281615227521201\n"
     ]
    }
   ],
   "source": [
    "print(rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+-11% of mistake "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Predictions for a single user** <br>\n",
    "Here user id 100 is selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_user = test.filter(test['UserID']==100).select([\"ArtistID\",\"UserID\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "recommendations = best_model.transform(single_user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+----------+\n",
      "|ArtistID|UserID|prediction|\n",
      "+--------+------+----------+\n",
      "| 1000276|   100|0.37870556|\n",
      "| 1045024|   100|0.32465968|\n",
      "| 1022226|   100| 0.2962859|\n",
      "| 1019547|   100|0.28094342|\n",
      "| 1046331|   100| 0.1974639|\n",
      "| 1046217|   100| 0.1912872|\n",
      "| 1024038|   100|0.14846405|\n",
      "+--------+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "recommendations.orderBy('prediction', ascending=False).show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
